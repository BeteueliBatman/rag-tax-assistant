import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin
import time
import json

class InfoHubScraper:
    def __init__(self):
        self.base_url = "https://infohub.rs.ge/ka"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def get_page_links(self, url):
        """áƒ’áƒ•áƒ”áƒ áƒ“áƒ˜áƒ“áƒáƒœ áƒ§áƒ•áƒ”áƒšáƒ áƒšáƒ˜áƒœáƒ™áƒ˜áƒ¡ áƒ›áƒáƒ«áƒ˜áƒ”áƒ‘áƒ"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            links = set()
            
            # áƒ•áƒ”áƒ«áƒ”áƒ‘áƒ— áƒ§áƒ•áƒ”áƒšáƒ áƒ¥áƒáƒ áƒ—áƒ£áƒš áƒ’áƒ•áƒ”áƒ áƒ“áƒ¡
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(self.base_url, href)
                
                # áƒ›áƒ®áƒáƒšáƒáƒ“ infohub.rs.ge/ka áƒšáƒ˜áƒœáƒ™áƒ”áƒ‘áƒ˜
                if full_url.startswith(self.base_url) and full_url != self.base_url:
                    links.add(full_url)
            
            return list(links)
        
        except Exception as e:
            print(f"Error fetching {url}: {e}")
            return []
    
    def scrape_page_content(self, url):
        """áƒ’áƒ•áƒ”áƒ áƒ“áƒ˜áƒ“áƒáƒœ áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒáƒ›áƒáƒ¦áƒ”áƒ‘áƒ"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # áƒ•áƒ¨áƒšáƒ˜áƒ— script, style, nav, footer áƒ¢áƒ”áƒ’áƒ”áƒ‘áƒ¡
            for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
                tag.decompose()
            
            # áƒ«áƒ˜áƒ áƒ˜áƒ—áƒáƒ“áƒ˜ áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ˜áƒ¡ áƒ›áƒáƒ«áƒ˜áƒ”áƒ‘áƒ
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_='content')
            
            if main_content:
                text = main_content.get_text(separator='\n', strip=True)
            else:
                text = soup.get_text(separator='\n', strip=True)
            
            # áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒ’áƒáƒ¬áƒ›áƒ”áƒœáƒ“áƒ
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            text = '\n'.join(lines)
            
            # Title-áƒ˜áƒ¡ áƒ›áƒáƒ«áƒ˜áƒ”áƒ‘áƒ
            title = soup.find('h1')
            title = title.get_text(strip=True) if title else url.split('/')[-1]
            
            return {
                'url': url,
                'title': title,
                'content': text
            }
        
        except Exception as e:
            print(f"Error scraping {url}: {e}")
            return None
    
    def scrape_all(self, max_pages=50):
        """áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ¡áƒ™áƒ áƒ”áƒ˜áƒáƒ˜áƒœáƒ’áƒ˜"""
        print("ğŸš€ Starting scraping process...")
        print(f"Base URL: {self.base_url}\n")
        
        # áƒ•áƒ˜áƒ¬áƒ§áƒ”áƒ‘áƒ— áƒ›áƒ—áƒáƒ•áƒáƒ áƒ˜ áƒ’áƒ•áƒ”áƒ áƒ“áƒ˜áƒ“áƒáƒœ
        to_visit = [self.base_url]
        visited = set()
        all_content = []
        
        while to_visit and len(visited) < max_pages:
            url = to_visit.pop(0)
            
            if url in visited:
                continue
            
            print(f"[{len(visited)+1}/{max_pages}] Scraping: {url}")
            visited.add(url)
            
            # áƒ•áƒ˜áƒ¦áƒ”áƒ‘áƒ— áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ¡
            content = self.scrape_page_content(url)
            
            if content and content['content']:
                all_content.append(content)
                print(f"  âœ“ Extracted {len(content['content'])} characters")
                
                # áƒ•áƒ˜áƒœáƒáƒ®áƒáƒ•áƒ— áƒªáƒáƒšáƒ™áƒ” áƒ¤áƒáƒ˜áƒšáƒ¨áƒ˜
                filename = f"page_{len(all_content):03d}.txt"
                filepath = os.path.join('data', 'raw', filename)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(f"URL: {content['url']}\n")
                    f.write(f"Title: {content['title']}\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(content['content'])
            
            # áƒ•áƒ”áƒ«áƒ”áƒ‘áƒ— áƒáƒ®áƒáƒš áƒšáƒ˜áƒœáƒ™áƒ”áƒ‘áƒ¡
            new_links = self.get_page_links(url)
            for link in new_links:
                if link not in visited and link not in to_visit:
                    to_visit.append(link)
            
            # áƒ—áƒáƒ•áƒáƒ–áƒ˜áƒáƒœáƒáƒ“ áƒ•áƒ”áƒšáƒáƒ“áƒ”áƒ‘áƒ˜áƒ— (1 áƒ¬áƒáƒ›áƒ˜ áƒ’áƒ•áƒ”áƒ áƒ“áƒ”áƒ‘áƒ¡ áƒ¨áƒáƒ áƒ˜áƒ¡)
            time.sleep(1)
        
        # áƒ•áƒ˜áƒœáƒáƒ®áƒáƒ•áƒ— metadata-áƒ¡
        metadata_file = os.path.join('data', 'raw', 'metadata.json')
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(all_content, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… Scraping completed!")
        print(f"ğŸ“„ Scraped {len(all_content)} pages")
        print(f"ğŸ’¾ Saved to data/raw/")
        
        return all_content

def main():
    # áƒ¡áƒáƒ¥áƒáƒ¦áƒáƒšáƒ“áƒ˜áƒ¡ áƒ¨áƒ”áƒ¥áƒ›áƒœáƒ
    os.makedirs('data/raw', exist_ok=True)
    
    # Scraper-áƒ˜áƒ¡ áƒ’áƒáƒ¨áƒ•áƒ”áƒ‘áƒ
    scraper = InfoHubScraper()
    results = scraper.scrape_all(max_pages=50)
    
    print(f"\nğŸ‰ Done! Scraped {len(results)} pages")

if __name__ == "__main__":
    main()