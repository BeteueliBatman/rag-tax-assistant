import os
import json
import re
from pathlib import Path
from typing import List, Dict

class TextProcessor:
    def __init__(self, chunk_size=800, chunk_overlap=200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def clean_text(self, text: str) -> str:
        """áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒ’áƒáƒ¬áƒ›áƒ”áƒœáƒ“áƒ"""
        # áƒ›áƒ áƒáƒ•áƒáƒšáƒ¯áƒ”áƒ áƒáƒ“áƒ˜ áƒ®áƒáƒ–áƒ˜áƒ¡ áƒ’áƒáƒ“áƒáƒ¢áƒáƒœáƒ”áƒ‘áƒ˜
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # áƒ–áƒ”áƒ“áƒ›áƒ”áƒ¢áƒ˜ áƒ¡áƒ˜áƒªáƒáƒ áƒ˜áƒ”áƒšáƒ”áƒ”áƒ‘áƒ˜
        text = re.sub(r' {2,}', ' ', text)
        
        # áƒ¡áƒ¢áƒ áƒ˜áƒ¥áƒáƒœáƒ˜áƒ¡ áƒ’áƒáƒ¬áƒ›áƒ”áƒœáƒ“áƒ
        text = text.strip()
        
        return text
    
    def split_into_chunks(self, text: str) -> List[str]:
        """áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ chunks-áƒáƒ“ áƒ“áƒáƒ§áƒáƒ¤áƒ"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            # áƒ—áƒ£ áƒ‘áƒáƒšáƒáƒ¨áƒ˜ áƒ•áƒáƒ áƒ—
            if end >= len(text):
                chunk = text[start:].strip()
                if chunk:
                    chunks.append(chunk)
                break
            
            # áƒ•áƒ”áƒ«áƒ”áƒ‘áƒ— áƒ¬áƒ”áƒ áƒ¢áƒ˜áƒšáƒ¡ áƒ áƒáƒ› áƒáƒ  áƒ’áƒáƒ•áƒ¬áƒ§áƒ•áƒ˜áƒ¢áƒáƒ— áƒ¬áƒ˜áƒœáƒáƒ“áƒáƒ“áƒ”áƒ‘áƒ
            chunk_end = end
            for separator in ['. ', '! ', '? ', '\n\n', '\n']:
                pos = text.rfind(separator, start, end)
                if pos != -1 and pos > start:
                    chunk_end = pos + len(separator)
                    break
            
            chunk_text = text[start:chunk_end].strip()
            if chunk_text:
                chunks.append(chunk_text)
            
            start = chunk_end - self.chunk_overlap
            if start < 0:
                start = chunk_end
        
        return chunks
    
    def process_all_files(self):
        """áƒ§áƒ•áƒ”áƒšáƒ áƒ¤áƒáƒ˜áƒšáƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ"""
        print("ğŸ“ Processing text files...")
        
        raw_dir = Path('data/raw')
        processed_dir = Path('data/processed')
        processed_dir.mkdir(exist_ok=True)
        
        # áƒ•áƒ¢áƒ•áƒ˜áƒ áƒ—áƒáƒ•áƒ— metadata-áƒ¡
        metadata_file = raw_dir / 'metadata.json'
        if not metadata_file.exists():
            print("âŒ Error: metadata.json not found!")
            print("   Please run: python src/scraper.py first")
            return []
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        all_chunks = []
        chunk_id = 0
        
        for i, doc in enumerate(metadata):
            print(f"\n[{i+1}/{len(metadata)}] Processing: {doc['title'][:50]}...")
            
            # áƒ•áƒ™áƒ˜áƒ—áƒ®áƒ£áƒšáƒáƒ‘áƒ— áƒ¤áƒáƒ˜áƒšáƒ¡
            filename = f"page_{i+1:03d}.txt"
            filepath = raw_dir / filename
            
            if not filepath.exists():
                print(f"  âš  File not found: {filename}")
                continue
            
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # áƒ•áƒ¬áƒ›áƒ”áƒœáƒ“áƒ—
            cleaned_text = self.clean_text(content)
            
            # áƒ•áƒ§áƒáƒ¤áƒ— chunks-áƒáƒ“
            chunks = self.split_into_chunks(cleaned_text)
            
            print(f"  âœ“ Created {len(chunks)} chunks")
            
            # áƒ•áƒáƒ›áƒáƒ¢áƒ”áƒ‘áƒ— metadata-áƒ¡
            for j, chunk_text in enumerate(chunks):
                all_chunks.append({
                    'id': chunk_id,
                    'text': chunk_text,
                    'source': doc['url'],
                    'title': doc['title'],
                    'chunk_index': j,
                    'total_chunks': len(chunks)
                })
                chunk_id += 1
        
        # áƒ•áƒ˜áƒœáƒáƒ®áƒáƒ•áƒ— chunks-áƒ¡
        chunks_file = processed_dir / 'chunks.json'
        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump(all_chunks, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… Processing completed!")
        print(f"ğŸ“¦ Total chunks: {len(all_chunks)}")
        print(f"ğŸ’¾ Saved to: {chunks_file}")
        
        return all_chunks

def main():
    processor = TextProcessor(chunk_size=800, chunk_overlap=200)
    chunks = processor.process_all_files()
    
    if chunks:
        # áƒ¡áƒ¢áƒáƒ¢áƒ˜áƒ¡áƒ¢áƒ˜áƒ™áƒ
        total_chars = sum(len(chunk['text']) for chunk in chunks)
        avg_chunk_size = total_chars / len(chunks) if chunks else 0
        
        print(f"\nğŸ“Š Statistics:")
        print(f"   Total chunks: {len(chunks)}")
        print(f"   Total characters: {total_chars:,}")
        print(f"   Average chunk size: {avg_chunk_size:.0f} characters")

if __name__ == "__main__":
    main()